{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1343143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Desktop\\Subtitle\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from  pprint import pprint\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import genanki\n",
    "from deep_translator import GoogleTranslator\n",
    "import stanza\n",
    "import os\n",
    "import deepl\n",
    "import hashlib, html\n",
    "from typing import Dict, List, Tuple\n",
    "import time, unicodedata as ud\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d3ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEPL_AUTH_KEY  = os.getenv('DEEPL_AUTH_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac00cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 12:05:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 18.0MB/s]                    \n",
      "2025-10-05 12:05:12 INFO: Downloaded file to C:\\Users\\szymo\\stanza_resources\\resources.json\n",
      "2025-10-05 12:05:13 INFO: Loading these models for language: sv (Swedish):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | talbanken          |\n",
      "| pos       | talbanken_charlm   |\n",
      "| lemma     | talbanken_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-10-05 12:05:13 INFO: Using device: cpu\n",
      "2025-10-05 12:05:13 INFO: Loading: tokenize\n",
      "2025-10-05 12:05:15 INFO: Loading: pos\n",
      "2025-10-05 12:05:17 INFO: Loading: lemma\n",
      "2025-10-05 12:05:17 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\szymo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "translator = deepl.Translator(DEEPL_AUTH_KEY)\n",
    "nlp = stanza.Pipeline(\"sv\", processors=\"tokenize,pos,lemma\")\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = stopwords.words(\"swedish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4b2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Known ambiguous terms → add a hint (and optional forced gloss)\n",
    "AMBIG = {\n",
    "    \"underkänd\":  {\"context\": \"School grading; means 'failed (an exam)'.\", \"override\": \"failed\"},\n",
    "    \"underkända\": {\"context\": \"School grading; means 'failed (an exam)'.\", \"override\": \"failed\"},\n",
    "    \"underkänt\":  {\"context\": \"School grading; means 'failed (an exam)'.\", \"override\": \"failed\"},\n",
    "    # add more false friends here...\n",
    "}\n",
    "\n",
    "# 2) Tiny domain detector (optional)\n",
    "DOMAINS = {\n",
    "    \"school\": {\n",
    "        \"keywords\": {\"prov\",\"betyg\",\"lärare\",\"skola\",\"elever\",\"kurs\",\"tentamen\"},\n",
    "        \"context\": \"School / grading context.\"\n",
    "    },\n",
    "    \"medical\": {\n",
    "        \"keywords\": {\"sjukhus\",\"läkare\",\"behandling\",\"symptom\",\"diagnos\"},\n",
    "        \"context\": \"Medical context.\"\n",
    "    },\n",
    "    \"finance\": {\n",
    "        \"keywords\": {\"bolag\",\"aktier\",\"börsen\",\"fakturor\",\"intäkter\",\"kostnader\"},\n",
    "        \"context\": \"Business / finance context.\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26333c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWEDISH_DELETE_WORDS = [\n",
    "    # greetings / interjections\n",
    "    \"hej\",\"hejsan\",\"hallå\",\"tjena\",\"tjenare\",\"tjenixen\",\"tja\",\"goddag\",\"godmorgon\",\"godkväll\",\"mors\",\n",
    "    \"aha\",\"oj\",\"åh\",\"hmm\",\"mm\",\"mmm\",\"eh\",\"öh\",\"öhm\",\"äh\",\"asså\",\"ba\",\n",
    "    # yes / no / acknowledgements\n",
    "    \"ja\",\"japp\",\"jo\",\"visst\",\"absolut\",\"okej\",\"ok\",\"okey\",\"nej\",\"icke\",\n",
    "    # fillers / discourse markers\n",
    "    \"liksom\",\"typ\",\"alltså\",\"ju\",\"väl\",\"likaså\",\"likväl\",\"så\",\"då\",\"bara\",\"redan\",\"också\",\"dessutom\",\"kanske\",\"nog\",\n",
    "    # pronouns / determiners\n",
    "    \"jag\",\"du\",\"han\",\"hon\",\"den\",\"det\",\"vi\",\"ni\",\"de\",\"mig\",\"dig\",\"honom\",\"henne\",\"oss\",\"er\",\"dem\",\"man\",\n",
    "    \"min\",\"mitt\",\"mina\",\"din\",\"ditt\",\"dina\",\"sin\",\"sitt\",\"sina\",\"vår\",\"vårt\",\"våra\",\"er\",\"ert\",\"era\",\n",
    "    \"denna\",\"detta\",\"dessa\",\"någon\",\"något\",\"några\",\"ingen\",\"inget\",\"inga\",\"vilken\",\"vilket\",\"vilka\",\"som\",\n",
    "    # common verbs/aux/modals\n",
    "    \"är\",\"var\",\"vara\",\"blir\",\"blev\",\"bli\",\"ha\",\"har\",\"hade\",\"gör\",\"gjorde\",\"göra\",\n",
    "    \"kan\",\"kunde\",\"ska\",\"skall\",\"skulle\",\"vill\",\"ville\",\"måste\",\"bör\",\"brukar\",\"får\",\"fick\",\n",
    "    # adverbs / particles\n",
    "    \"inte\",\"aldrig\",\"alltid\",\"ofta\",\"ibland\",\"sällan\",\"här\",\"där\",\"hit\",\"dit\",\"hem\",\"borta\",\"nu\",\"sen\",\"snart\",\"igen\",\"än\",\n",
    "    \"mycket\",\"lite\",\"mer\",\"mest\",\"mindre\",\"minst\",\"kvar\",\"både\",\"antingen\",\"heller\",\"också\",\n",
    "    # prepositions\n",
    "    \"i\",\"på\",\"till\",\"från\",\"för\",\"med\",\"utan\",\"över\",\"under\",\"mellan\",\"genom\",\"mot\",\"bland\",\"hos\",\n",
    "    \"före\",\"efter\",\"kring\",\"runt\",\"enligt\",\"trots\",\"vid\",\"omkring\",\"om\",\"åt\",\"av\",\"per\",\"cirka\",\"ca\",\n",
    "    # conjunctions / subjunctions\n",
    "    \"och\",\"men\",\"eller\",\"samt\",\"utan\",\"att\",\"för att\",\"eftersom\",\"därför\",\"medan\",\"när\",\"innan\",\"efter att\",\"om\",\n",
    "    \"fast\",\"så att\",\"såväl\",\"både\",\"dock\",\"ty\",\"varför\",\n",
    "\n",
    "    'ska', 'nej', 'hej','bra', 'ja','vill','lite', 'jaha', 'wow'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e3e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['eddie', 'martin', 'william', 'lisa','eddies', 'patrik', 'bianca', \"katja\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c5ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = stopwords + SWEDISH_DELETE_WORDS + names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6701dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name: str):\n",
    "    with open(file_name, 'r', encoding = 'UTF-8') as f:\n",
    "        file = f.read()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7dfe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line, for_word):\n",
    "    line = line.replace('-','')\n",
    "    line = line.replace('...','')\n",
    "    line = line.strip()\n",
    "\n",
    "    if line.endswith(','):\n",
    "        line = line [:-1]\n",
    "        \n",
    "    if for_word:\n",
    "        line = re.sub(r'\\p{P}+', ' ', line)\n",
    "        line = line.lower()\n",
    "\n",
    "    line = line.replace('  ', ' ')\n",
    "    line = line.strip()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d698c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file, for_word = False):\n",
    "    text = file.split('\\n')\n",
    "    text = [piece.strip() for txt in text for piece in txt.split('.') if piece.strip()]\n",
    "\n",
    "    filtered = []\n",
    "    for line in text:\n",
    "        if len(line) <= 1:\n",
    "            continue\n",
    "        if re.findall('^\\d{2}', line):\n",
    "            continuez\n",
    "        line = clean_line(line, for_word)\n",
    "        \n",
    "        line = ' '.join(line.split())\n",
    "        filtered.append(line)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efd4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SENT_END_RE = re.compile(\n",
    "    r\"\"\"\n",
    "    (?<!\\b(?:dr|prof|mr|mrs|ms|nr|itp|np|tj|kap|art|al)\\.)   # variable-width lookbehind OK here\n",
    "    (?<=\\.|!|\\?|…)\n",
    "    [\"')\\]]*\n",
    "    \\s+\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "# SRT timestamp line: 00:00:12,345 --> 00:00:14,567\n",
    "_SRT_TIME_RE = re.compile(\n",
    "    r'^\\d{2}:\\d{2}:\\d{2}[,\\.]\\d{3}\\s*-->\\s*\\d{2}:\\d{2}:\\d{2}[,\\.]\\d{3}$'\n",
    ")\n",
    "\n",
    "def _split_sentences(text: str) -> list[str]:\n",
    "    return [s.strip() for s in _SENT_END_RE.split(text) if s.strip()]\n",
    "\n",
    "def clean_data(file: str, for_word: bool = False) -> list[str]:\n",
    "    # 1) Strip SRT artifacts, collapse to a single text\n",
    "    lines = []\n",
    "    for raw in file.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.isdigit():               # SRT index lines\n",
    "            continue\n",
    "        if _SRT_TIME_RE.match(line):     # SRT time range lines\n",
    "            continue\n",
    "        lines.append(line)\n",
    "    text = \" \".join(lines)\n",
    "\n",
    "    # 2) Split by sentences (not lines)\n",
    "    sentences = _split_sentences(text)\n",
    "\n",
    "    # 3) Clean and filter\n",
    "    filtered = []\n",
    "    for s in sentences:\n",
    "        s = clean_line(s, for_word)      # your cleaner\n",
    "        s = \" \".join(s.split())\n",
    "        if len(s) > 1:\n",
    "            filtered.append(s)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f14bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_words(lines:list) -> list:\n",
    "    full_list = []\n",
    "    for line in lines:\n",
    "        for word in line.split(' '):\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            if isinstance(word, str):\n",
    "                full_list.append(word)\n",
    "    \n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c9fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram(words_tokenized, n, min_count):\n",
    "    generated_ngrams = ngrams(words_tokenized, n)\n",
    "    counter_grams = Counter(list(generated_ngrams))\n",
    "    counter_dict = dict(counter_grams)\n",
    "    return {k: v for k,v in counter_dict.items() if v >= min_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9241c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_ngrams(words_tokenized, min_counts):\n",
    "    grams = {}\n",
    "    gram_counts = {}\n",
    "    for i in range(3,10):\n",
    "        if generate_ngram(words_tokenized, i, min_counts):\n",
    "            grams[i] = generate_ngram(words_tokenized, i, min_counts)\n",
    "            gram_counts[i] = len(grams[i])\n",
    "        else:\n",
    "            break\n",
    "    return grams, gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f127d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art(word):\n",
    "    doc = nlp(word)\n",
    "    for sent in doc.sentences:\n",
    "       for w in sent.words:\n",
    "           if w.upos == \"NOUN\":\n",
    "               feats = w.feats or \"\"          # e.g. \"Definite=Ind|Gender=Neut|Number=Sing\"\n",
    "               art = \"en\" if \"Gender=Com\" in feats else (\"ett\" if \"Gender=Neut\" in feats else None)\n",
    "               return art\n",
    "           \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9633ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(lemma_count, picked_by_pos):\n",
    "    total_tokens = sum(cnt for d in lemma_count.values() for cnt in d.values())\n",
    "    covered = set()\n",
    "    for pos, d in picked_by_pos.items():\n",
    "        covered |= {(pos, lemma) for lemma in d}\n",
    "    # sum using best matching POS counts\n",
    "    covered_tokens = 0\n",
    "    for pos, d in picked_by_pos.items():\n",
    "        for lemma, cnt in d.items():\n",
    "            covered_tokens += lemma_count.get(pos, {}).get(lemma, 0)\n",
    "    return covered_tokens / total_tokens if total_tokens else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e536ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deck_size_for_target(lemma_count: dict, target_pct: float, allowed_pos: set | None = None):\n",
    "    \"\"\"\n",
    "    Return (k, achieved_pct) where k is the smallest number of (pos, lemma) items\n",
    "    needed to reach target_pct coverage. Uses your per-(pos, lemma) counts.\n",
    "    target_pct can be 0–1 (e.g., 0.8) or 0–100 (e.g., 80).\n",
    "    \"\"\"\n",
    "    # normalize target to 0..1\n",
    "    tp = target_pct / 100.0 if target_pct > 1 else float(target_pct)\n",
    "\n",
    "    # total tokens across all POS/lemmas (same definition you use in coverage())\n",
    "    total_tokens = sum(cnt for d in lemma_count.values() for cnt in d.values())\n",
    "    if total_tokens == 0 or tp <= 0:\n",
    "        return 0, 0.0\n",
    "    if tp >= 1:\n",
    "        # full deck size = all unique (pos, lemma)\n",
    "        full_k = sum(len(d) for pos, d in lemma_count.items() if not allowed_pos or pos in allowed_pos)\n",
    "        return full_k, 1.0\n",
    "\n",
    "    # flatten and rank by frequency desc (respect optional POS filter)\n",
    "    items = [\n",
    "        (pos, lemma, cnt)\n",
    "        for pos, d in lemma_count.items() if (not allowed_pos or pos in allowed_pos)\n",
    "        for lemma, cnt in d.items()\n",
    "    ]\n",
    "    items.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    covered_tokens = 0\n",
    "    k = 0\n",
    "    # (pos, lemma) uniqueness is inherent; no need for an extra set unless inputs repeat\n",
    "    for pos, lemma, cnt in items:\n",
    "        covered_tokens += cnt\n",
    "        k += 1\n",
    "        achieved = covered_tokens / total_tokens\n",
    "        if achieved >= tp:\n",
    "            return k, achieved\n",
    "\n",
    "    # If target not met (e.g., tp > achievable due to filters), return max\n",
    "    return k, covered_tokens / total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a2f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_quota(lemma_count, target_total=250, quotas=None):\n",
    "    \"\"\"\n",
    "    lemma_count: dict like {'NOUN': {'barn':12, 'dag':4, ...}, 'VERB': {...}, ...}\n",
    "    target_total: total number of lemmas you want\n",
    "    quotas: POS -> fraction, e.g. {'VERB':0.35,'NOUN':0.40,'ADJ':0.15,'ADV':0.10}\n",
    "            If None, distribute evenly across POS present.\n",
    "    Returns: (study_list, picked_by_pos)\n",
    "      study_list = [(lemma, POS, count)] ordered by selection stage\n",
    "      picked_by_pos = {'NOUN': {'barn':12, ...}, 'VERB': {...}, ...}\n",
    "    \"\"\"\n",
    "    # Convert inner dicts to Counters\n",
    "    pos_counters = {pos: Counter(d) for pos, d in lemma_count.items()}\n",
    "    all_pos = list(pos_counters.keys())\n",
    "\n",
    "    if not quotas:\n",
    "        quotas = {pos: 1/len(all_pos) for pos in all_pos}\n",
    "\n",
    "    # translate fractions to integer quotas, then backfill any shortfall\n",
    "    raw = {pos: int(target_total * quotas.get(pos, 0)) for pos in all_pos}\n",
    "    short = target_total - sum(raw.values())\n",
    "    # give leftover slots to the biggest buckets by available items\n",
    "    fill_order = sorted(all_pos, key=lambda p: sum(pos_counters[p].values()), reverse=True)\n",
    "    i = 0\n",
    "    while short > 0 and fill_order:\n",
    "        pos = fill_order[i % len(fill_order)]\n",
    "        raw[pos] += 1\n",
    "        short -= 1\n",
    "        i += 1\n",
    "\n",
    "    picked = set()\n",
    "    picked_by_pos = defaultdict(dict)\n",
    "    study_list = []\n",
    "\n",
    "    # 1) take top-k per POS by its quota\n",
    "    for pos, k in raw.items():\n",
    "        for lemma, cnt in pos_counters[pos].most_common():\n",
    "            if len(picked_by_pos[pos]) >= k:\n",
    "                break\n",
    "            if lemma in picked:\n",
    "                continue\n",
    "            picked.add(lemma)\n",
    "            picked_by_pos[pos][lemma] = cnt\n",
    "            study_list.append((lemma))\n",
    "\n",
    "    # 2) backfill if some POS had too few items or overlaps reduced selection\n",
    "    if len(study_list) < target_total:\n",
    "        # overall ranking across all POS\n",
    "        overall = Counter()\n",
    "        per_pos_for_lemma = defaultdict(dict)\n",
    "        for pos, C in pos_counters.items():\n",
    "            for lemma, cnt in C.items():\n",
    "                overall[lemma] += cnt\n",
    "                per_pos_for_lemma[lemma][pos] = cnt\n",
    "\n",
    "        for lemma, _ in overall.most_common():\n",
    "            if len(study_list) >= target_total:\n",
    "                break\n",
    "            if lemma in picked:\n",
    "                continue\n",
    "            # choose the POS where this lemma is most frequent\n",
    "            pos = max(per_pos_for_lemma[lemma].items(), key=lambda x: x[1])[0]\n",
    "            cnt = per_pos_for_lemma[lemma][pos]\n",
    "            picked.add(lemma)\n",
    "            picked_by_pos[pos][lemma] = cnt\n",
    "            study_list.append((lemma))\n",
    "\n",
    "    return study_list, picked_by_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b91bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_sentences(cleaned_file):\n",
    "    sentence_clean = {}\n",
    "    for line in cleaned_file:\n",
    "        sentence_clean[clean_line(line, for_word = True)] = line\n",
    "\n",
    "    return sentence_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00326057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_grams_with_sentences(grams, sentence_clean):\n",
    "    di = {}\n",
    "    # sentence_clean: {clean -> original}\n",
    "    for n, content in grams.items():\n",
    "        for gram, _ in content.items():\n",
    "            key = ' '.join(gram)\n",
    "            pat = re.compile(rf\"\\b{re.escape(key)}\\b\", flags=re.IGNORECASE)\n",
    "            di[gram] = [cent_raw for sent, cent_raw in sentence_clean.items() if pat.search(sent)]\n",
    "    return di\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e029d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    doc = nlp(word)\n",
    "    out = []\n",
    "    for sent in doc.sentences:\n",
    "        for w in sent.words:\n",
    "            if w.upos == \"NOUN\":\n",
    "                feats = w.feats or \"\"          # e.g. \"Definite=Ind|Gender=Neut|Number=Sing\"\n",
    "                art = \"en\" if \"Gender=Com\" in feats else (\"ett\" if \"Gender=Neut\" in feats else None)\n",
    "                return [art, w.upos , w.lemma]\n",
    "            else:\n",
    "                return [None, w.upos, w.lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de92230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    final_words = defaultdict(dict)\n",
    "    for w in words:\n",
    "        art, pos, lem = get_lemma(w)\n",
    "        info = final_words.setdefault(lem, {\"Artikel\": art, \"POS\": pos, \"Forms\": set()})\n",
    "        info[\"Forms\"].add(w)\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c00ed289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_count(words, lemmatized):\n",
    "    surface_to_row = {}\n",
    "    for lem, info in lemmatized.items():\n",
    "        for form in info[\"Forms\"]:\n",
    "            surface_to_row[form] = (info[\"Artikel\"], info[\"POS\"], lem)\n",
    "\n",
    "    counts = defaultdict(Counter)\n",
    "    for w in words:\n",
    "        row = surface_to_row.get(w)\n",
    "        if not row: \n",
    "            continue\n",
    "        _, upos, lemma = row\n",
    "        counts[upos][lemma] += 1\n",
    "    return {pos: dict(cnt) for pos, cnt in counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de9ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_shortest_by_lemma(\n",
    "    final: Dict[str, Dict[str, List[str]]],\n",
    "    prefer_inflected: bool = True,      # prefer forms where word_form != lemma\n",
    "    measure: str = \"tokens\"             # \"tokens\" or \"chars\"\n",
    ") -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (lemma, chosen_word_form, shortest_example_sentence).\n",
    "    Chooses per lemma the word form whose shortest example is the shortest.\n",
    "    \"\"\"\n",
    "\n",
    "    def key_for(s: str):\n",
    "        # primary: token count, secondary: char length\n",
    "        return (len(s.split()), len(s)) if measure == \"tokens\" else (len(s),)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for lemma, forms in final.items():\n",
    "        forms['to_study'] = {}\n",
    "        if not forms:\n",
    "            continue\n",
    "\n",
    "        candidates = []\n",
    "        for form, sents in forms['examples'].items():\n",
    "            if not sents:\n",
    "                continue\n",
    "            shortest_sent_for_form = min(sents, key=key_for)\n",
    "\n",
    "            # Rank: 0 = inflected preferred, 1 = base (if prefer_inflected)\n",
    "            rank = 0 if (prefer_inflected and form != lemma) else 1\n",
    "            candidates.append((rank, key_for(shortest_sent_for_form), form, shortest_sent_for_form))\n",
    "\n",
    "        if not candidates:\n",
    "            continue\n",
    "        \n",
    "        # Choose minimal by (rank, length-key)\n",
    "        _, _, best_form, best_sentence = min(candidates, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        forms['to_study']['word'] = best_form\n",
    "        forms['to_study']['sentence'] = best_sentence\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b72f6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_example(lematized, sentences):\n",
    "    res = defaultdict(list)\n",
    "    for word, lemma_data in lematized.items():\n",
    "        for word_form in lemma_data['Forms']:\n",
    "            res[word].append(word_form) \n",
    "\n",
    "    for lemma, words in res.items():\n",
    "        lemma_map = {}  \n",
    "        for word in set(words):\n",
    "            pattern = re.compile(rf\"\\b{re.escape(word)}\\b\", flags=re.IGNORECASE)\n",
    "            hits = [target for sent, target in sentences.items() if pattern.search(sent)]\n",
    "            if hits:\n",
    "                lemma_map[word] = hits\n",
    "        if lemma_map:\n",
    "            lematized[lemma]['examples'] = lemma_map\n",
    "    return lematized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2262b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d27f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CACHE = {}\n",
    "def _tokenize_sv(s: str) -> set[str]:\n",
    "    s = ud.normalize(\"NFC\", s.lower())\n",
    "    return set(re.findall(r\"[a-zåäöéüøß\\-]+\", s))\n",
    "\n",
    "def guess_context_sv(sv_sentence: str) -> str | None:\n",
    "    tokens = _tokenize_sv(sv_sentence)\n",
    "    for dom in DOMAINS.values():  # first match wins\n",
    "        if tokens & dom[\"keywords\"]:\n",
    "            return dom[\"context\"]\n",
    "    return None\n",
    "\n",
    "def tag_first(s, target):\n",
    "    # case-insensitive, whole-word; preserves original casing in the sentence\n",
    "    pattern = re.compile(rf\"\\b{re.escape(target)}\\b\", flags=re.IGNORECASE)\n",
    "    return pattern.sub(lambda m: \"<term>\"+m.group(0)+\"</term>\", s, count=1)\n",
    "\n",
    "def extract_term(en_text: str) -> str:\n",
    "    a, b = en_text.find(\"<term>\"), en_text.find(\"</term>\")\n",
    "    if a != -1 and b != -1 and b > a:\n",
    "        return en_text[a+6:b]\n",
    "    a, b = en_text.find(\"&lt;term&gt;\"), en_text.find(\"&lt;/term&gt;\")\n",
    "    if a != -1 and b != -1 and b > a:\n",
    "        return en_text[a+12:b]\n",
    "    return \"\"\n",
    "\n",
    "def translate_tagged(sv_sentence: str, target: str, translator) -> tuple[str, str]:\n",
    "    key = (sv_sentence, target)\n",
    "    if key in _CACHE:\n",
    "        return _CACHE[key]\n",
    "\n",
    "    # 1) Build an optional context\n",
    "    ctx = None\n",
    "    amb = AMBIG.get(target.lower())\n",
    "    if amb:\n",
    "        ctx = amb[\"context\"]\n",
    "    if ctx is None:\n",
    "        ctx = guess_context_sv(sv_sentence)\n",
    "\n",
    "    # 2) Tag first occurrence and call DeepL (one quick retry on 429)\n",
    "    tagged = tag_first(sv_sentence, target)\n",
    "    kwargs = dict(\n",
    "        source_lang=\"SV\", target_lang=\"EN-GB\",\n",
    "        tag_handling=\"xml\", non_splitting_tags=[\"term\"],\n",
    "        preserve_formatting=True, outline_detection=False\n",
    "    )\n",
    "    if ctx:  # only pass when we have one\n",
    "        kwargs[\"context\"] = ctx\n",
    "\n",
    "    try:\n",
    "        res = translator.translate_text(tagged, **kwargs)\n",
    "    except deepl.TooManyRequestsException:\n",
    "        time.sleep(3)\n",
    "        res = translator.translate_text(tagged, **kwargs)\n",
    "\n",
    "    en_sentence = res.text\n",
    "    word_eng = extract_term(en_sentence)\n",
    "\n",
    "    # 3) Optional last-resort override for known false friends\n",
    "    if amb and amb.get(\"override\"):\n",
    "        en_sentence = re.sub(r\"(<term>)(.*?)(</term>)\",\n",
    "                             r\"\\1\"+amb[\"override\"]+r\"\\3\",\n",
    "                             en_sentence, count=1, flags=re.DOTALL)\n",
    "        word_eng = amb[\"override\"]\n",
    "\n",
    "    _CACHE[key] = (en_sentence, word_eng)\n",
    "    return _CACHE[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0807e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_first(s, target):\n",
    "    # case-insensitive, whole-word; preserves original casing in the sentence\n",
    "    pattern = re.compile(rf\"\\b{re.escape(target)}\\b\", flags=re.IGNORECASE)\n",
    "    return pattern.sub(lambda m: \"<term>\"+m.group(0)+\"</term>\", s, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89241e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abasd <term>b</term> c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_first('abasd b c','b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e72f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a<term>b</term>asd b c'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_first('abasd b c','b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "386a9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- utils ---\n",
    "def _bold_term_tags(s: str) -> str:\n",
    "    # raw <term>…</term>\n",
    "    s = s.replace(\"<term>\", \"<b>\").replace(\"</term>\", \"</b>\")\n",
    "    # escaped &lt;term&gt;…&lt;/term&gt;\n",
    "    s = s.replace(\"&lt;term&gt;\", \"<b>\").replace(\"&lt;/term&gt;\", \"</b>\")\n",
    "    return s\n",
    "\n",
    "def _highlight_once(sentence: str, target: str) -> str:\n",
    "    # bold FIRST whole-word target (case-insensitive), preserving original case\n",
    "    pat = re.compile(rf\"\\b{re.escape(target)}\\b\", re.IGNORECASE)\n",
    "    return pat.sub(lambda m: f\"<b>{m.group(0)}</b>\", sentence, count=1)\n",
    "\n",
    "# --- FRONT ---\n",
    "def _front_text(rec: dict) -> str:\n",
    "    \"\"\"\n",
    "    Front shows:\n",
    "      1) English sentence (italic, with bolded term)\n",
    "      2) 'gloss (pos)'\n",
    "    \"\"\"\n",
    "    ts = rec.get('to_study', {}) or {}\n",
    "    en_sent = ts.get('Sentence_translated') or ''\n",
    "    gloss   = ts.get('Word_translated') or ''   # <-- fixed source\n",
    "    pos     = (rec.get('POS') or '').lower()\n",
    "\n",
    "    # clean \"None\"\n",
    "    if gloss == 'None': gloss = ''\n",
    "    if en_sent == 'None': en_sent = ''\n",
    "\n",
    "    en_sent = _bold_term_tags(en_sent)\n",
    "\n",
    "    gloss_line = f\"{gloss}  ({pos})\" if gloss and pos else (gloss or (f\"({pos})\" if pos else \"\"))\n",
    "\n",
    "    parts = []\n",
    "    if en_sent:\n",
    "        parts.append(f\"<div style='font-style:italic'>{en_sent}</div>\")\n",
    "    if gloss_line:\n",
    "        parts.append(f\"<div style='margin-top:6px'>{html.escape(gloss_line)}</div>\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "# --- BACK & CARD BUILDER ---\n",
    "def build_card(rec: dict) -> tuple[str, str]:\n",
    "    pos   = rec.get('POS', '') or ''\n",
    "    art   = rec.get('Artikel') or ''   # 'en' / 'ett' / ''\n",
    "    ts    = rec.get('to_study', {}) or {}\n",
    "    word  = ts.get('Word', '') or ''\n",
    "    sv    = ts.get('Sentence', '') or ''\n",
    "    en    = ts.get('Sentence_translated', '') or ''\n",
    "    en    = _bold_term_tags(en)\n",
    "\n",
    "    front = _front_text(rec)\n",
    "\n",
    "    badge = (\n",
    "        f\"<span style='background:#eee;border-radius:6px;padding:2px 6px;margin-left:6px'>{art}</span>\"\n",
    "        if (pos == 'NOUN' and art) else \"\"\n",
    "    )\n",
    "\n",
    "    back = (\n",
    "        f\"<div style='font-size:1.35em;line-height:1.2'><b>{html.escape(word)}</b>{badge}</div>\"\n",
    "        f\"<div style='margin-top:8px'>{_highlight_once(html.escape(sv), word)}</div>\"\n",
    "        f\"<div style='margin-top:6px;font-style:italic'>{en}</div>\"\n",
    "        f\"<div style='margin-top:6px;color:#777'>{pos.lower()}</div>\"\n",
    "    )\n",
    "    return front, back\n",
    "\n",
    "# --- DECK ---\n",
    "def _note_guid(word: str, sv: str) -> str:\n",
    "    h = hashlib.sha1(f\"{word}||{sv}\".encode('utf-8')).hexdigest()\n",
    "    return h\n",
    "\n",
    "def generate_deck(name: str, db: dict) -> genanki.Deck:\n",
    "    model = genanki.Model(\n",
    "        1607392319,  # keep stable once chosen\n",
    "        'EN→SV Minimal',\n",
    "        fields=[{'name': 'Front'}, {'name': 'Back'}],\n",
    "        templates=[{\n",
    "            'name': 'Card 1',\n",
    "            'qfmt': '{{Front}}',\n",
    "            'afmt': '{{FrontSide}}<hr id=\"answer\">{{Back}}',\n",
    "        }],\n",
    "        css=\"\"\"\n",
    "        .card { font-family: Inter, Arial; font-size: 18px; line-height: 1.4; }\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    deck = genanki.Deck(2059200110, name)\n",
    "\n",
    "    for _, rec in sorted(db.items()):\n",
    "        front, back = build_card(rec)\n",
    "        word = (rec.get('to_study', {}) or {}).get('Word', '')\n",
    "        sv   = (rec.get('to_study', {}) or {}).get('Sentence', '')\n",
    "        if not front or not word:\n",
    "            continue\n",
    "        note = genanki.Note(\n",
    "            model=model,\n",
    "            fields=[front, back],\n",
    "            guid=_note_guid(word, sv),\n",
    "        )\n",
    "        deck.add_note(note)\n",
    "\n",
    "    return deck\n",
    "\n",
    "def save_deck(deck: genanki.Deck, filename: str):\n",
    "    genanki.Package(deck).write_to_file(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea3a5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fingerprint(norm_text: str) -> str:\n",
    "    # get hash SHA-256\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63344fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(lines: list[str]) -> str:\n",
    "    cleaned_file_words = clean_data(lines, for_word= True)\n",
    "    cleaned_words = convert_to_words(cleaned_file_words)\n",
    "    # Retained punctuation\n",
    "    cleaned_file = clean_data(lines, for_word= False)\n",
    "    sentence_clean = get_cleaned_sentences(cleaned_file)\n",
    "    return sentence_clean, cleaned_words\n",
    "\n",
    "def get_grams(words: list[str], sentence_clean, cleaned_file_words) -> dict[str,list]:\n",
    "    grams, _ = generate_multiple_ngrams(cleaned_file_words, 3)\n",
    "    matched_dict = match_grams_with_sentences(grams, sentence_clean)\n",
    "    return matched_dict\n",
    "\n",
    "def get_coverage_info(lemma_count):\n",
    "    coverage_info = {}\n",
    "    for coverage in range(5, 101, 5):\n",
    "        size, _ = deck_size_for_target(lemma_count,coverage)\n",
    "        coverage_info[coverage] = size\n",
    "    return coverage_info\n",
    "\n",
    "def translate_deck(study_list, final_with_picked_sentences):\n",
    "    for word in study_list:\n",
    "        content = final_with_picked_sentences[word]\n",
    "        sv  = content['to_study']['sentence']           # keep original casing\n",
    "        tgt = content['to_study']['word']               # original form; tagger is case-insensitive\n",
    "        en_sentence, word_eng = translate_tagged(sv, tgt, translator)\n",
    "        content['to_study']['sentence_translated'] = en_sentence\n",
    "        content['to_study']['word_translated'] = word_eng\n",
    "        time.sleep(0.7)  # pacing; adjust to your plan's limits\n",
    "    return final_with_picked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0470d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(file_content: str) -> str:\n",
    "    \"\"\"Compute SHA-256 hash of file content for caching\"\"\"\n",
    "    return hashlib.sha256(file_content.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadc046",
   "metadata": {},
   "source": [
    "First stage:\n",
    "1 - 6 minutes\n",
    "1. Ingest data\n",
    "2. Normalize data -> get cleaned sentences and clean words\n",
    "3. Get n-grams for sentences and cleaned words\n",
    "4. Lematize, get counts, pick examples, \n",
    "Second stage:\n",
    "10-30 minutes\n",
    "1. Configure deck content (token coverage etc.)\n",
    "2. Translate final deck \n",
    "3. Generate deck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline_state(episode_id: str, stage: str, data: dict):\n",
    "    \"\"\"Save pipeline state to Redis\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_data(file_hash:str) -> dict:\n",
    "    \"\"\" Return data from cachce\"\"\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2a36105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_state(file_hash: str, stage: str) -> dict | None:\n",
    "    \"\"\"Load pipeline state from Redis using file hash\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage1_data_generation(episode_id: str, file_content:str):\n",
    "    file_hash = compute_file_hash(file_content)\n",
    "\n",
    "    cached_data = get_cached_data(file_hash)\n",
    "\n",
    "    if cached_data:\n",
    "        print(\"I WILL USE CACHED DATA FROM SUPABASE AND OMMIT PROCESSING\")\n",
    "        return\n",
    "    sentence_clean, cleaned_file_words = normalize_text(file)\n",
    "    get_grams = get_grams(cleaned_file_words, sentence_clean)\n",
    "    words_clean = [w for w in cleaned_file_words if (w not in to_delete)]\n",
    "    lematized = lemmatize_words(words_clean)\n",
    "    lemma_count = get_lemma_count(words_clean, lematized)\n",
    "    final_with_sentences = get_sentence_example(lematized, sentence_clean)\n",
    "    final_with_picked_sentences = pick_shortest_by_lemma(final_with_sentences, prefer_inflected=True, measure=\"tokens\")\n",
    "    stage1_data = {\n",
    "        'episode_data_processed':final_with_picked_sentences,\n",
    "        'lemma_count': lemma_count\n",
    "    }\n",
    "    \n",
    "    return stage1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41fd750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated token coverage: 67.4%\n"
     ]
    }
   ],
   "source": [
    "# SECOND STAGE\n",
    "quotas = {\"VERB\": 0.35, \"NOUN\": 0.40, \"ADJ\": 0.15, \"ADV\": 0.10}\n",
    "study_list, picked_by_pos = select_top_quota(lemma_count, target_total=260, quotas=quotas)\n",
    "cov = coverage(lemma_count)\n",
    "get_coverage_info(lemma_count)\n",
    "print(f\"Estimated token coverage: {cov:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
